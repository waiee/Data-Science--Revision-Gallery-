{"cells":[{"cell_type":"markdown","metadata":{"id":"b_byWWSKOBfp"},"source":["# Lab07 - Region Segmentation\n","### TDS3651 Visual Information Processing\n"]},{"cell_type":"markdown","metadata":{"id":"rqMrK2kzOBf9"},"source":["This lab will guide you on how to perform region segmentation on images from different perspectives. The simple method relies on colour information, where objects or regions can be segmented by clustering pixel values, a step higher than just thresholding. More powerful graph-based algorithms such as Felzenswalb's method and SLIC superpixels can also be used to segment regions using a variety of affinities between pixels such as brightness, color, textures, etc. In addition, to extend our current understanding of feature representations, we can now attempt to extract features from these segments (instead of individual pixels) to construct feature histograms, which can be used for matching images.\n","\n","In addition to the typical libraries, we will use several libraries from [scikit-image](http://scikit-image.org/) package to enable us to perform segmentation with minimal coding overhead."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NCzPyTWAOBfu"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt\n","\n","%matplotlib inline"]},{"cell_type":"markdown","source":["## Recap: Segmentation by color\n","In the previous color processing lab, the HSV color space were used with a range thresholding to perform segmentation. However, it is noticeable that the segmentations are not perfect and it is challenging to determine the appropriate thresholds."],"metadata":{"id":"9xD9G5NWUyGv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GJwea5H9OBf9"},"outputs":[],"source":["aao = cv2.imread('apples_oranges.jpg')\n","aao_hsv = cv2.cvtColor(aao, cv2.COLOR_BGR2HSV)\n","\n","lowerhue = 0#5 # fill in\n","upperhue = 25#20 # fill in\n","lower_orange = np.array([lowerhue,225,150])\n","upper_orange = np.array([upperhue,255,255])\n","mask = cv2.inRange(aao_hsv, lower_orange, upper_orange)\n","res = cv2.bitwise_and(aao, aao, mask=mask)\n","\n","fig, axs = plt.subplots(1, 5, figsize=(10,10))\n","channels = ['H', 'S', 'V']\n","axs[0].imshow(cv2.cvtColor(aao, cv2.COLOR_BGR2RGB))\n","axs[0].axis('off')\n","axs[0].set_title('RGB Image')\n","for i, ax in enumerate(axs[1:-1]):\n","    ax.imshow(aao_hsv[:,:,i], cmap='hsv')\n","    ax.axis('off')\n","    ax.set_title(channels[i])\n","axs[-1].imshow(cv2.cvtColor(res, cv2.COLOR_BGR2RGB))\n","axs[-1].axis('off')\n","axs[-1].set_title('Segmentation')\n","plt.show()\n","plt.close()"]},{"cell_type":"markdown","metadata":{"id":"wLSi-5eqOBf-"},"source":["## Segmentation by Clustering Pixels\n","\n","Another way to segment regions based on pixel intensities is to perform clustering. Clustering is able to group together pixels that are of the same intensity range. This may seem trivial but it is a step of improvement over thresholding.\n","\n","Using the k-means and vector quantization functions from SciPy package."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qhtFQDpgOBf-"},"outputs":[],"source":["# read image and reshape appropriately to MxN array (M observations, N-dim each)\n","aao = cv2.imread('apples_oranges.jpg')\n","r = aao.shape[0]\n","c = aao.shape[1]\n","N = aao.shape[2]\n","aors = np.reshape(aao, (r*c, N))\n","print(aao.shape, aors.shape)"]},{"cell_type":"markdown","metadata":{"id":"7N-CJmAHOBf_"},"source":["Let's say we want to find 3 clusters containing different color intensities, we shall set $k=3$ so that clustering will group them into 3 groups. Like before, use `kmeans` to generate the codebook and then vector quantization (VQ) to assign each observation to its \"code\" or the cluster label (in this case 0, 1, 2)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zny_1URSOBgA"},"outputs":[],"source":["k = 3\n","# k-means algo: data in MxN: M observations, N features\n","codebook, _ = kmeans(np.float32(aors), k)\n","\n","# k by N codebook, showing k centroids of N-dimensions\n","print(codebook)\n","\n","# VQ assigns each observation to its \"code\", which is its cluster label\n","code, _ = vq(np.float32(aors), codebook)\n","print(np.max(code))  # k=3 means labels are 0, 1, 2\n","print(code.shape)\n","\n","# reshape back to image size\n","aorsrb = np.reshape(code, (r, c))\n","print(aorsrb.shape)"]},{"cell_type":"markdown","metadata":{"id":"n2XfvfyrOBgA"},"source":["After reshaping back, we expect the three regions to show different colors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qGuUc3BMOBgB"},"outputs":[],"source":["# show image with the assigned labels\n","ao_rgb = cv2.cvtColor(aao, cv2.COLOR_BGR2RGB)\n","plt.subplot(121), plt.imshow(ao_rgb)\n","plt.subplot(122), plt.imshow(np.uint8(aorsrb), cmap='jet')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9zF43gecOBgB"},"source":["**Q1**: Try with more clusters and less clusters. Do you think it is possible to know the number of clusters to use in advance?\n","\n","What do you think are the weaknesses of this method?"]},{"cell_type":"markdown","source":[],"metadata":{"id":"SyS1E1xVA3Px"}},{"cell_type":"markdown","metadata":{"id":"dxJZbC55OBgC"},"source":["## Graph-based methods: Felzenszwalb's method and SLIC superpixels\n","Let's start off by loading requried libraries and an image of a particularly colorful animal..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qioSdocbOBgC"},"outputs":[],"source":["from skimage.segmentation import felzenszwalb, slic, random_walker\n","from skimage.segmentation import mark_boundaries, find_boundaries, relabel_sequential\n","\n","img = cv2.imread('parrot.jpg')\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","plt.imshow(img), plt.xticks([]), plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"u0rClf9POBgC"},"source":["Running Felzenszwalb's algorithm on the image is really simple with scikit-image's [segmentation](http://scikit-image.org/docs/dev/api/skimage.segmentation.html) library."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mZRRPuQDOBgD"},"outputs":[],"source":["segments_fz = felzenszwalb(img, scale=70, sigma=0.5, min_size=150)\n","print(\"Felzenszwalb number of segments: {}\".format(len(np.unique(segments_fz))))"]},{"cell_type":"markdown","metadata":{"id":"DhuiQksxOBgD"},"source":["The array `segments_fz` contains the labeled pixels after segmentation. Visualize these segmented regions using `imshow` with a spectral colormap."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MY2EUmYAOBgD"},"outputs":[],"source":["plt.imshow(segments_fz, cmap='jet')\n","plt.show()\n","print(segments_fz)"]},{"cell_type":"markdown","metadata":{"id":"kKpJ9D9QOBgD"},"source":["To visualize the boundaries of the segments, there's also a function `mark_boundaries` to do just that. Observe how the segments are determined, and if they make sense."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ybve95ktOBgE"},"outputs":[],"source":["fz_boundaries = mark_boundaries(img, segments_fz)\n","plt.imshow(fz_boundaries)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9lpFx1H_OBgE"},"source":["**Q2**: Let's compute the mean colour for each segment and display the \"colorized\" segments (just like in the picture shown below). ![FZ-colorized parrot](parrot_felz.png)\n","\n","**Hint**: A simple way to perform the colorization is using the [label2rgb](https://scikit-image.org/docs/dev/api/skimage.color.html#skimage.color.label2rgb) from the skimage.color libray."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l23QERh-OBgE"},"outputs":[],"source":["from skimage.color import label2rgb\n"]},{"cell_type":"markdown","metadata":{"id":"n9XLQMUkOBgF"},"source":["**Q3**: Perform the same thing again (segment, mark boundaries, colorized) using SLIC superpixels instead.\n","\n","![SLIC-colorized parrot](slic_colorized.png)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7PF9zCwbOBgF"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"bIYpM8J_OBgG"},"source":["### Side track: 3D Plots\n","\n","If you have completed Q2.3, you should already have your mean colours in an array.\n","\n","Here's some code to plot the mean color data (RGB) on a 3D axes to visualize their locations in a 3-dimensional coordinate system. (Documentation: See [here](http://matplotlib.org/mpl_toolkits/mplot3d/tutorial.html#mpl_toolkits.mplot3d.Axes3D.scatter) on the 3D scatter plot)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FfDg9mWFOBgH"},"outputs":[],"source":["from mpl_toolkits.mplot3d import Axes3D\n","\n","colormeans = []\n","for i in range(np.max(segments_fz)+1):\n","    colormeans.append(np.mean(img[np.nonzero(segments_fz==i)], axis=0))\n","\n","%matplotlib notebook\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(111, projection='3d')\n","xs, ys, zs,  = list(zip(*colormeans))\n","ax.scatter(xs, ys, zs, c='b', marker='o')\n","ax.set_xlabel('R')\n","ax.set_ylabel('G')\n","ax.set_zlabel('B')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wsp4M99dOBgK"},"outputs":[],"source":["%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"ReSxMskAOBgL"},"source":["From the scatter plot, looks like we have a pretty colour picture. There are many independent points scattered all over the RGB spectrum."]},{"cell_type":"markdown","metadata":{"id":"JmI6ZxikOBgL"},"source":["## Using segmented regions to extract features\n","What SLIC offers (that is better than Felzenszwalb's method can) is the uniformity of the segments. Not exactly uniform or similar in size, but *as similar or uniform as possible*. This allows us to do further things such as use them to extract features from each segment more consistently. Imagine if your segments are from very diverse sizes, even the very large segments would only be represented by the same number of features as smaller segments.\n","\n","Of course, the easiest and most straightforward way of extracting \"higher-level\" features from each segment is by simply taking the mean colour value (just a single value) as representative of each segment.\n"]},{"cell_type":"markdown","metadata":{"id":"d-_kcHT7OBgM"},"source":["**Q4**: Next, attempt to construct a colour histogram by binning the R, G, and B channel values into a number of bins, and concatenating them up. Determine on your own account how many bins to use for each channel. Write a function that takes in an image filename, and outputs the color histogram constructed from the segmented regions. Decide on your own if you need more input parameters to provide more options when generating the histogram. You can do this in Spyder as it can be easier to run the whole bunch of codes there.\n","\n","You are now given a number of other animal images -- camel, llama, red panda, and a second parrot.\n","\n","<img src=\"redpanda.jpg\" style=\"display:inline; height:150px\"> <img src=\"camel.jpg\" style=\"display:inline; height:150px\"> <img src=\"llama.jpg\" style=\"display:inline; height:150px\"> <img src=\"parrot2.jpg\" style=\"display:inline; height:150px\">\n","\n","**Tip**: Considering that these different images may produce slightly different number of superpixels (although SLIC allows you to fix the number of segments, but the algorithm can only give you an approximation), it is advisable to normalize the colour histogram counts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-ZG2OIfcOBgM"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"EPHpXYWAOBgN"},"source":["**Q5**: Find the nearest match to the `parrot.jpg` image from among these 4 other images. Determine which distance measure to use. (Fingers crossed that it will be `parrot2.jpg`!)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEcm9nZvOBgN"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"-3BtuLliOBgO"},"source":["## Additional Exercises\n"]},{"cell_type":"markdown","metadata":{"id":"_mRSAtNNOBgO"},"source":["**Q1**: **GrabCut**: Interactive Foreground Extraction\n","\n","Look at the following link: [http://docs.opencv.org/3.1.0/d8/d83/tutorial_py_grabcut.html](http://docs.opencv.org/3.1.0/d8/d83/tutorial_py_grabcut.html) and have a go.\n","\n","Although it says \"interactive\", it can be easily modified to work automatically (no humans needed)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TuzTW0zxOBgP"},"outputs":[],"source":[]}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}