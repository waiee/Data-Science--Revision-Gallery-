{"cells":[{"cell_type":"markdown","metadata":{"id":"hhYIsYjFqXsu"},"source":["# Lab03 - Filtering and Gradients / Edges\n","### TDS3651 Visual Information Processing\n"]},{"cell_type":"markdown","metadata":{"id":"qMoR0hmqqXsw"},"source":["## II. Neighborhood Processing\n","\n","## Image Smoothing"]},{"cell_type":"markdown","metadata":{"id":"GCGZhJ5IqXsx"},"source":["Image smoothing is achieved by convolving an image with a smoothing filter kernel. Some sources refer to these type of kernels as a *low-pass* filter kernel. Generally, smoothing an image removes noises but it also removes other \"high frequency\" content that may be important, such as edges and ridges from the image structure. \"Low frequency\" content are allowed to \"pass\" or kept.\n","\n","In all, OpenCV provides four built-in smoothing/blurring techniques."]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"uriXLgnp-iay","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1700387501357,"user_tz":-480,"elapsed":22265,"user":{"displayName":"Noramiza Binti Hashim","userId":"14760452241051619060"}},"outputId":"f7bfe537-01a7-432d-a753-2ab0ba45b909"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["#insert your own path\n","path ='/content/drive/MyDrive/Labs/Lab03/'"],"metadata":{"id":"m6UWetDd-h6S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RMYSRgr5qXsz"},"source":["### Built-in Blurring Functions\n","\n","Next, let's read the `redflower.jpg` image we have previously used. For colour images, remember to convert the channel ordering from BGR to RGB so that `matplotlib` display functions show the right colours."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21CslaZiqXs0"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VlF-0nddqXs8"},"outputs":[],"source":["img = cv2.imread(path+'redflower.jpg')\n","img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","plt.imshow(img)\n","plt.xticks([]), plt.yticks([])\n","plt.show()\n","\n","# If you use OpenCV's imshow function (which pops up a separate window instead), you should not convert from BGR to RGB\n","# because it takes care of the channel ordering automatically\n","#    cv2.imshow('img',img)\n","#    cv2.waitKey(0)\n","#    cv2.destroyAllWindows()\n","\n"]},{"cell_type":"code","source":["blur1 = cv2.blur(img,(5,5))     # 5x5 kernel filter, try other values"],"metadata":{"id":"4XaCeTSoFB1T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8bRoLk2CqXtD"},"source":["This blurs the image using a 5x5 averaging filter, define as:\n","$$H = \\frac{1}{25}\\begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\end{bmatrix}$$\n","\n","Display the two images side-by-side in subplots,"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i5zkVPVQqXtF"},"outputs":[],"source":["plt.subplot(121),plt.imshow(img),plt.title('Original')\n","plt.xticks([]), plt.yticks([])\n","plt.subplot(122),plt.imshow(blur1),plt.title('Blurred')\n","plt.xticks([]), plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"aSqoV312qXtM"},"source":["The same effect can be achieved using [`cv2.boxFilter()`](http://docs.opencv.org/3.1.0/d4/d86/group__imgproc__filter.html#gad533230ebf2d42509547d514f7d3fbc3). Try it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"exdsRMdlqXtN"},"outputs":[],"source":["blur0 = cv2.boxFilter(img,-1,(5,5))\n","plt.imshow(blur0),plt.title('Blurred')\n","plt.xticks([]), plt.yticks([])\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tq_GXziVqXtU"},"outputs":[],"source":["blur2 = cv2.GaussianBlur(img,(5,5),0)"]},{"cell_type":"markdown","metadata":{"id":"5aKop72eqXtb"},"source":["Smoothing with Gaussian filter can be performed using the function, [`cv2.GaussianBlur()`](http://docs.opencv.org/3.1.0/d4/d86/group__imgproc__filter.html#gaabe8c836e97159a9193fb0b11ac52cf1). We should specify the `width` and `height` of kernel, which should be positive and odd. We also should specify the standard deviation in X and Y direction, `sigmaX` and `sigmaY` respectively. If only `sigmaX` is specified, `sigmaY` is taken as same as `sigmaX`. If both are given as zeros, they are calculated from the kernel size. Gaussian blurring is a highly effective method for removing gaussian noise from the image. To obtain the Gaussian kernel, you can use the function [`cv2.GaussianKernel()`](http://docs.opencv.org/3.1.0/d4/d86/group__imgproc__filter.html#gac05a120c1ae92a6060dd0db190a61afa)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NRk39gxtqXtc"},"outputs":[],"source":["plt.figure(figsize=(10,5))\n","plt.subplot(131),plt.imshow(img),plt.title('Original')\n","plt.xticks([]), plt.yticks([])\n","plt.subplot(132),plt.imshow(blur1),plt.title('Averaging Filter')\n","plt.xticks([]), plt.yticks([])\n","plt.subplot(133),plt.imshow(blur2),plt.title('Gaussian Filter')\n","plt.xticks([]), plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"72ND3IZxqXtj"},"source":["The median filter is a non-linear filter that takes the median of all pixels under the kernel neighborhood and replaces the central element with this median value. In most filtering procedures, the central element is a newly calculated value which may be a new pixel value. But in median blurring, the central element is always replaced by some pixel value in the image within the defined neighborhood. This reduces the noise effectively, particularly salt-and-pepper noise. Its kernel size should be a positive odd integer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4Fu-GRZdqXtl"},"outputs":[],"source":["blur3 = cv2.medianBlur(img,5)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HuO-mykxqXtr"},"outputs":[],"source":["plt.figure(figsize=(12,6))\n","plt.subplot(141),plt.imshow(img),plt.title('Original')\n","plt.xticks([]), plt.yticks([])\n","plt.subplot(142),plt.imshow(blur1),plt.title('Averaging Filter')\n","plt.xticks([]), plt.yticks([])\n","plt.subplot(143),plt.imshow(blur2),plt.title('Gaussian Filter')\n","plt.xticks([]), plt.yticks([])\n","plt.subplot(144),plt.imshow(blur3),plt.title('Median Filter')\n","plt.xticks([]), plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rS95DHLaqXtx"},"source":["What can you observe from the outcome of all three smoothing filters (all have same kernel size of 5x5)?"]},{"cell_type":"markdown","metadata":{"id":"m6vVlwJUqXtz"},"source":["**Q1**: You are given two noise-ridden images of Lena. Use appropriate filters (with appropriate kernel size and parameters) to remove the noise as best as you could."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DKsPm6znqXt0"},"outputs":[],"source":["lena_noisy1 = cv2.imread(path+'lena_gaussianNoise.png')\n","lena_noisy2 = cv2.imread(path+'lena_spNoise.png')\n","\n","# Add your code here\n"]},{"cell_type":"markdown","metadata":{"id":"Imi5nmClqXt6"},"source":["### Defining Kernels\n","\n","What if we wish to define our own filters? OpenCV provides a function [`cv2.filter2D()`](http://docs.opencv.org/3.1.0/d4/d86/group__imgproc__filter.html#ga27c049795ce870216ddfb366086b5a04) to convolve a kernel with an image. So, all we need to do is to define the filter in array form.\n","\n","Let's try to manually create a 5x5 averaging filter kernel (which is similar to that done earlier),\n","$$H = \\frac{1}{25}\\begin{bmatrix} 1 & 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\\\ 1 & 1 & 1 & 1 & 1 \\end{bmatrix}$$\n","and then convolve it with an image to perform filtering."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NiJ2TX1BqXt7"},"outputs":[],"source":["kernel = np.ones((5,5),np.float32)/25\n","\n","filtered = cv2.filter2D(img,-1,kernel) # find out from doc what these parameters mean...also, there are other parameters not used\n","\n","plt.figure(figsize=(10,5))\n","plt.subplot(131), plt.imshow(img), plt.title('Original')\n","plt.subplot(132), plt.imshow(filtered), plt.title('Using defined kernel')\n","plt.subplot(133), plt.imshow(blur1), plt.title('Using cv2.blur')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"0qNQaP7pqXuD"},"source":["**Q2**: How about trying out these other kernels?\n","$$H_1 = \\frac{1}{16}\\begin{bmatrix} 1 & 2 & 1 \\\\ 2 & 4 & 2 \\\\ 1 & 2 & 1 \\end{bmatrix}$$\n","$$H_2 = \\frac{1}{25}\\begin{bmatrix} 0 & 0 & 1 & 0 & 0 \\\\ 0 & 2 & 2 & 2 & 0 \\\\ 1 & 2 & 5 & 2 & 1 \\\\ 0 & 2 & 2 & 2 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\end{bmatrix}$$\n","\n","Explore other kernels and see how it affect the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VH56Ro6cqXuE"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"AySzjisJqXuK"},"source":["## Image Sharpening\n","\n","Sharpening is the opposite process of smoothing, where the details of an image are accentuated to produce a sharpened image. This process can be accomplished by first obtaining the \"details\" of an image, by subtracting a smoothened image from the original image. Then, these \"details\" can be added back to the original image to produce the sharpened output image.\n","\n","Let's first generate a blur version of the `lena.jpg` image, and we shall attempt to sharpen it."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N6WWyzV4qXuL"},"outputs":[],"source":["lena = cv2.imread(path+'lena.jpg', cv2.IMREAD_GRAYSCALE)\n","blurlena = cv2.GaussianBlur(lena,(13,13),0)   # let's make it blurrer than usual"]},{"cell_type":"markdown","metadata":{"id":"KsAJmeCNqXuQ"},"source":["Next, we shall perform sharpening."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y80V_sK6qXuS"},"outputs":[],"source":["sm = cv2.blur(blurlena,(11,11))\n","details = blurlena - sm;\n","shp = blurlena + details;\n","\n","plt.figure(figsize=(10,5))\n","plt.subplot(131), plt.imshow(blurlena, cmap = 'gray')\n","plt.xticks([]), plt.yticks([])\n","plt.subplot(132), plt.imshow(details, cmap = 'gray')\n","plt.xticks([]), plt.yticks([])\n","plt.subplot(133), plt.imshow(shp, cmap = 'gray')\n","plt.xticks([]), plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"-3SJg19gqXuZ"},"source":["**Q3**: Try sharpening the `redflower` image (which is a colour image). What can you observe?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrWJV9I_qXub"},"outputs":[],"source":["#read image\n","#conver bgr to rgb\n","#convert image data type to float32\n","#do blurring\n","\n","#get details by subtracting blur image from original\n","#add details to original\n","\n","#clip image values between 0 to 255\n","# convert back to uint8"]},{"cell_type":"markdown","metadata":{"id":"YHps1TZXqXuh"},"source":["### Padding before Filtering\n","\n","Explore [`numpy.pad`](https://docs.scipy.org/doc/numpy/reference/generated/numpy.pad.html) function to try out different padding styles which can have a varying degree of effect on the image border after filtering."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"r0s2pR4-qXui"},"outputs":[],"source":["lenapadded = np.pad(lena, (3,3), 'constant', constant_values=0)   # pad with zeros\n","plt.imshow(lenapadded, cmap='gray')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"c9rGtt27qXuq"},"source":["**Q4**: Do the earlier operations using `cv2.blur`, `cv2.filter2D`, etc. perform any padding during filtering? Investigate this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TAoqR7_cqXus"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"tTEyAH2sqXux"},"source":["## II. Gradients / Edges\n","This section has two parts, The first introduces image gradients and how they are used for edge detection, and also more well known edge detectors like Canny. Secondly, we take a look at how binary images can be processed to extract information valuable for further interpretation. The concept of connected components is a crucial one to understand how segmented regions can be labeled and counted."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K4gmUkQQqXuy"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from matplotlib import pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"W_HdLVKKqXu4"},"source":["Let's read the `redflower.jpg` image we have previously used. Gradients are the intensity changes in an image that can constitute edges. Typically, gradients can be computed directly from grayscale images instead of colour images, as information of these intensity changes are still inherent after conversion.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RNbblkU6qXu5"},"outputs":[],"source":["img = cv2.imread(path+'redflower.jpg', cv2.IMREAD_GRAYSCALE)\n","kernel_y = np.array([[1],[-1]])\n","kernel_x = np.transpose(kernel_y)\n","print(kernel_x), print(kernel_y)\n","gradient_x = cv2.filter2D(img, cv2.CV_32F, kernel_x)    # CV_32F sets the filtered values to 32-bit floats.\n","gradient_y = cv2.filter2D(img, cv2.CV_32F, kernel_y)\n","\n","plt.figure(figsize=(10,5))\n","plt.subplot(131), plt.imshow(img, cmap='gray'), plt.title('Original')\n","plt.subplot(132), plt.imshow(gradient_x, cmap='gray'), plt.title('Gradient along x')\n","plt.subplot(133), plt.imshow(gradient_y, cmap='gray'), plt.title('Gradient along y')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"90WyrpPMqXu_"},"source":["You will notice vertical-like edges appearing when we filter with kernel $\\begin{bmatrix} 1 & -1 \\end{bmatrix}$ while horizontal-like edges appear when we filter with kernel $\\begin{bmatrix} 1 \\\\ -1 \\end{bmatrix}$.\n","\n","Yes, this is correct. Try to make sense of why these kernels result in these type of edges (vertical/horizontal)."]},{"cell_type":"markdown","metadata":{"id":"9IwiI4nUqXvA"},"source":["Let's compute the magnitude of these gradient values. There are two ways of calculating (assuming $G_x$ denote gradient along $x$ while $G_y$ denote gradient along $y$:\n","1. The \"correct\" way: $\\sqrt{{G_x}^2 + {G_y}^2}$\n","2. The \"fast\" way: $\\mid{G_x}\\mid + \\mid{G_y}\\mid$\n","\n","Observe if there's any difference in the two gradient magnitude images.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vcCNvRvQqXvB"},"outputs":[],"source":["mag1 = np.sqrt((gradient_x ** 2) + (gradient_y ** 2))\n","mag2 = np.abs(gradient_x) + np.abs(gradient_y)\n","plt.figure(figsize=(8,4))\n","plt.subplot(121), plt.imshow(mag1, cmap='gray')\n","plt.subplot(122), plt.imshow(mag2, cmap='gray')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"ySyRDnLaqXvH"},"source":["Let's compute the orientation of these gradient values:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-m5GOxoPqXvJ"},"outputs":[],"source":["import math\n","direction = np.arctan2(gradient_y,gradient_x)    # add eps to prevent division by zero\n","direction_angle = np.rad2deg(direction)\n","fig = plt.imshow(direction_angle, cmap='jet')\n","cbar = plt.colorbar(fig)     # creates a colorbar\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"oJQgFQccqXvO"},"source":["OpenCV provides three types of gradient filters: Sobel, Scharr and Laplacian. We will try the Laplacian and Sobel filters.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"pVEE5EkxqXvP"},"source":["### Sobel filters\n","\n","Sobel operators essentially consists of a joint Gausssian smoothing plus differentiation operation, so it is more resistant towards noise. You can specify the direction of derivatives to be taken, vertical or horizontal. You can also specify the size of kernel by the argument `ksize`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iWbmVEcgqXvR"},"outputs":[],"source":["sobelx = cv2.Sobel(img,cv2.CV_8U,1,0,ksize=1)\n","sobely = cv2.Sobel(img,cv2.CV_8U,0,1,ksize=1)\n","\n","sobelxy = cv2.Sobel(img,cv2.CV_8U,1,1,ksize=1)\n","\n","plt.imshow(sobelxy,cmap='gray')"]},{"cell_type":"markdown","metadata":{"id":"LMwi2xCOqXvZ"},"source":["### Laplacian (2nd order gradient) filters\n","\n","It calculates the Laplacian of the image given by the relation, $\\Delta=\\frac{\\delta^2 f}{\\delta x^2}+\\frac{\\delta^2 f}{\\delta y^2}$ where each derivative is found using Sobel derivatives. If ksize = 1, then following kernel is used for filtering:\n","$\\begin{bmatrix} 0 & 1 & 0 \\\\ 1 & -4 & 1 \\\\ 0 & 1 & 0 \\end{bmatrix}$\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HH0vg9ItqXva"},"outputs":[],"source":["laplacian = cv2.Laplacian(img,cv2.CV_8U)"]},{"cell_type":"markdown","metadata":{"id":"02dwYi2DqXvg"},"source":["Let's show them all in a single plot:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hNT7d5DWqXvh"},"outputs":[],"source":["plt.figure(figsize=(10,10))\n","plt.subplot(2,2,1),plt.imshow(img,cmap = 'gray')\n","plt.title('Original'), plt.xticks([]), plt.yticks([])\n","plt.subplot(2,2,2),plt.imshow(laplacian,cmap = 'gray')\n","plt.title('Laplacian'), plt.xticks([]), plt.yticks([])\n","plt.subplot(2,2,3),plt.imshow(sobelxy,cmap = 'gray')\n","plt.title('Sobel X'), plt.xticks([]), plt.yticks([])\n","plt.subplot(2,2,4),plt.imshow(sobely,cmap = 'gray')\n","plt.title('Sobel Y'), plt.xticks([]), plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_txVQH07qXvn"},"source":["**Q5**: What do you observe from the edges of the Sobel edge detector and the Laplacian edge detector?"]},{"cell_type":"markdown","metadata":{"id":"dhbEQfPGqXvo"},"source":["Type your answer here:"]},{"cell_type":"markdown","metadata":{"id":"U01w-PsZqXvq"},"source":["Previously, the output datatype used is `cv2.CV_8U` or `np.uint8`. But there is a slight problem with that. Black-to-White transition is taken as positive slope (it has a positive value) while White-to-Black transition is taken as a negative slope (it has negative value). So when `np.uint8` is used, all negative slopes are clipped to zero. In simple words, those edges are missed.\n","\n","If you want to detect both edges, better option is to keep the output datatype to some signed integer or floats, like `cv2.CV_16S`, `cv2.CV_64F` etc., take its absolute value and then convert back to `cv2.CV_8U`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TvqW31HAqXvr","scrolled":true},"outputs":[],"source":["shape_img = cv2.imread(path+'monitor.jpg',0)\n","\n","# Output dtype = cv2.CV_8U\n","sobelx8u = cv2.Sobel(shape_img,cv2.CV_8U,1,0,ksize=1)\n","\n","# Output dtype = cv2.CV_64F. Then take its absolute and convert to cv2.CV_8U\n","sobelx64f = cv2.Sobel(shape_img,cv2.CV_64F,1,0,ksize=1)\n","abs_sobel64f = np.absolute(sobelx64f)\n","sobel_8u = np.uint8(abs_sobel64f)\n","\n","plt.figure(figsize=(10,5))\n","plt.subplot(1,3,1), plt.imshow(shape_img,cmap = 'gray')\n","plt.title('Original'), plt.xticks([]), plt.yticks([])\n","plt.subplot(1,3,2), plt.imshow(sobelx8u,cmap = 'gray')\n","plt.title('Sobel CV_8U'), plt.xticks([]), plt.yticks([])\n","plt.subplot(1,3,3), plt.imshow(sobel_8u,cmap = 'gray')\n","plt.title('Sobel abs(CV_64F)'), plt.xticks([]), plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_nut9lEaqXvv"},"source":["### Canny edge detector"]},{"cell_type":"markdown","metadata":{"id":"2TrBU6YMqXvw"},"source":["OpenCV has the famous Canny edge detector built-in as a single function, [`cv2.Canny()`](http://docs.opencv.org/3.1.0/dd/d1a/group__imgproc__feature.html#ga04723e007ed888ddf11d9ba04e2232de).\n","\n","**`void cv::Canny  (   InputArray \timage,\n","                    OutputArray \tedges,\n","                    double \tthreshold1,\n","                    double \tthreshold2,\n","                    int \tapertureSize = 3,\n","                    bool \tL2gradient = false\n","                )\t`**\n","\n","We will see how to use it. First argument is our input image. Second and third arguments are our minVal and maxVal respectively. Third argument is aperture_size. It is the size of Sobel kernel used for find image gradients. By default this value is 3. Last argument is `L2gradient` which specifies the equation for finding gradient magnitude. If it is True, it uses the equation mentioned above which is more accurate, otherwise it uses the faster version: $G=|G_x|+|G_y|$. By default, it is False."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fL607Xu6qXvx","scrolled":true},"outputs":[],"source":["flower_edges = cv2.Canny(img,100,200)\n","monitor_edges = cv2.Canny(shape_img,100,200)\n","\n","plt.figure(figsize=(6,6))\n","plt.subplot(221),plt.imshow(shape_img,cmap = 'gray')\n","plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n","plt.subplot(222),plt.imshow(monitor_edges,cmap = 'gray')\n","plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n","plt.subplot(223),plt.imshow(img,cmap = 'gray')\n","plt.title('Original Image'), plt.xticks([]), plt.yticks([])\n","plt.subplot(224),plt.imshow(flower_edges,cmap = 'gray')\n","plt.title('Edge Image'), plt.xticks([]), plt.yticks([])\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9-HGB0PAqXv1"},"source":["## Additional Exercises"]},{"cell_type":"markdown","metadata":{"id":"dZSA99okqXv2"},"source":["**Q1. Alpha-trimmed mean filter**. Implement a function to perform this. You can test it using the blurred Lena image we used earlier.\n","\n","Hint: We need two loops to slide the window (corresponding to the kernel neighborhood) across the entire image\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_eKvWJ7cqXv3"},"outputs":[],"source":["def alphaTrimmedMean(im,alpha,n=5):\n","    # im: input image\n","    # alpha: alpha coefficient (between 0 and 1) defining the proportion of elements to be trimmed\n","    # n: default kernel size (default value 5)\n","\n","\n","    k = int((n-1)/2)   # half a kernel window. This value will be useful!\n","\n","    # COMPLETE THE REST OF THE FUNCTION -----------------------------------------------------------\n","    # pad with k pixels on all sides\n","\n","\n","    # create output image filled with zeros (preparation)\n","\n","\n","    # calculate the amount of trim, b --> the number of elements to trim from each end\n","\n","\n","    # process the image, pixel by pixel\n","    for i in range(k,im.shape[0]):\n","        for j in range(k,im.shape[1]):\n","\n","            # extract the window area\n","            block = im[max(i-k,0):min(i+k,im.shape[0]), max(j-k,0):min(j+k,im.shape[1])]\n","\n","            # reshape the neighborhood into a vector by flattening the 2D block\n","            wB = block.flatten()\n","\n","            # sort the neighborhood vector in ascending order\n","\n","\n","            # trim b elements from each end of the vector\n","\n","\n","            # calculate mean of the trimmed vector\n","\n","\n","            # assign the calculated mean value to the central pixel location\n","\n","    return img\n","\n","alpha = 0.1\n","enh_lena = alphaTrimmedMean(blurlena, alpha)\n","plt.xticks([]), plt.yticks([])\n","plt.imshow(enh_lena, cmap='gray'), plt.title('Alpha = %.2f'%alpha), plt.show()"]},{"cell_type":"markdown","metadata":{"id":"BnPsBO-OqXv7"},"source":["**Q2. Bilateral Filtering**\n","\n","OpenCV has a function for Bilateral Filtering called [`cv2.bilateralFilter()`](https://docs.opencv.org/3.4.1/d4/d86/group__imgproc__filter.html#ga9d7064d478c95d60003cf839430737ed)\n","\n","Try it out on the `cat.png` image.\n","\n","<img src=\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQHi_xmpgVkIkcq1X2UY4McVURozTKTfI3gEQ&usqp=CAU\">"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-j8uGJ-hqXv7"},"outputs":[],"source":["pan = cv2.imread('cat.png')\n","pan = cv2.blur(pan,(5,5))\n","# add code\n","\n","\n"]}],"metadata":{"anaconda-cloud":{},"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.5"}},"nbformat":4,"nbformat_minor":0}