{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LqKbMrP_y0rN"
   },
   "source": [
    "<img src=\"https://www.mmu.edu.my/fci/wp-content/uploads/2021/01/FCI_wNEW_MMU_LOGO.png\" style=\"height: 80px;\" align=left>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SSA2TMXxy0rZ"
   },
   "source": [
    "# Learning Objectives\n",
    "\n",
    "Towards the end of this lesson, you should be able to:\n",
    "- handle different types of missing data\n",
    "- handle noisy data using various techniques\n",
    "- merge different sets of data into a unified set\n",
    "- assess relationships of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "GEVMeWFey0rb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6RNqOK-ccT6i"
   },
   "source": [
    "---\n",
    "\n",
    "### For Google Colab Use Only\n",
    "Skip this section if you are using Jupyter Notebook etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "DE6XffsuclLs"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jcx6xDmdco5S"
   },
   "outputs": [],
   "source": [
    "drive_path = '/content/drive/MyDrive/Trimester/2310/TDS3301/Tutorials/Tutorial 3/' #set your google drive path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iE_qb2SFy0rf"
   },
   "source": [
    "\n",
    "---\n",
    "## Dealing with missing values\n",
    "\n",
    "Recap, when dealing with any dataset, read the data dictionary/description/metadata of your dataset. Missing values are often dealt with as `empty` cells in the data but sometimes, they are filled with dummy values like `?`, `-`, `.`, etc.\n",
    "If you don't have access to the metadata, look at the descriptive statistics (mean, min, max) or make use of visualizations to try and make sense of the data.\n",
    "\n",
    "For simple cases, consider this toy data here containing age and gender with missing data represented by `NaN`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "FE8hAsdTy0rg"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>NaN</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  gender\n",
       "0  18.0    male\n",
       "1  18.0    male\n",
       "2  19.0  female\n",
       "3  20.0  female\n",
       "4   NaN    male\n",
       "5  14.0  female\n",
       "6   NaN    male"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "age = pd.Series([18,18,19,20,np.nan,14,np.nan])\n",
    "gender = pd.Series(['male', 'male', 'female', 'female', 'male', 'female', 'male'])\n",
    "\n",
    "df = pd.DataFrame({'age':age, 'gender':gender})\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gNeARkGO0TYR"
   },
   "source": [
    "If missing data is \"recorded\" as `NaN` like in this case, then `Pandas` provide various useful functions to detect, remove, or fill (impute) them: <br>\n",
    "`df.isna()`: calculates the amount of missing data for each attribute<br>\n",
    "`df.dropna()`: drop rows of data containing missing data<br>\n",
    "`df.fillna()`: fill in missing data with set value.\n",
    "\n",
    "Try the following tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Uv61NwJJy0rl"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age       2\n",
       "gender    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the number of NAs in df by column\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "4u0Rh95Cy0rq"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18.0</td>\n",
       "      <td>male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.0</td>\n",
       "      <td>female</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    age  gender\n",
       "0  18.0    male\n",
       "1  18.0    male\n",
       "2  19.0  female\n",
       "3  20.0  female\n",
       "5  14.0  female"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a copy of df and name it df1, then drop all na.\n",
    "df1 = df.copy()\n",
    "df1.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a copy of df and name it df2, then fill NA with median of each feature\n",
    "#df3 = df.copy()\n",
    "#median = df.median(1)\n",
    "#df3.fillna(median)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4YfliPH2b-z"
   },
   "outputs": [],
   "source": [
    "# create a copy of df and name it df3, then fill NA with median of each feature\n",
    "# hint: use relevant function to extract the needed statistical measure\n",
    "df3 = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EptSqRrA0uxh"
   },
   "source": [
    "There are more ways to deal with missing values and for varieties of data such as categorical attributes. <br>\n",
    "Various imputation methods has been developed to make the \"choice\" of values to be more reflective of real data (fill with most frequent, MICE, etc.).\n",
    "Explore more here: https://scikit-learn.org/stable/modules/impute.html\n",
    "\n",
    "Let's try more advanced methods of imputation using the [Credit Approval dataset retrived from UCI ML repository](https://archive.ics.uci.edu/dataset/27/credit+approval). A copy of the dataset is provided with this tutorial materials.\n",
    "\n",
    "After understanding the information, proceed with reading the data and identifying the missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "sxpMlQZf_VXc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>A11</th>\n",
       "      <th>A12</th>\n",
       "      <th>A13</th>\n",
       "      <th>A14</th>\n",
       "      <th>A15</th>\n",
       "      <th>A16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b</td>\n",
       "      <td>30.83</td>\n",
       "      <td>0.000</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.25</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00202</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a</td>\n",
       "      <td>58.67</td>\n",
       "      <td>4.460</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>3.04</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>6</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00043</td>\n",
       "      <td>560</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a</td>\n",
       "      <td>24.50</td>\n",
       "      <td>0.500</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>q</td>\n",
       "      <td>h</td>\n",
       "      <td>1.50</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>824</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b</td>\n",
       "      <td>27.83</td>\n",
       "      <td>1.540</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>3.75</td>\n",
       "      <td>t</td>\n",
       "      <td>t</td>\n",
       "      <td>5</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00100</td>\n",
       "      <td>3</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b</td>\n",
       "      <td>20.17</td>\n",
       "      <td>5.625</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>w</td>\n",
       "      <td>v</td>\n",
       "      <td>1.71</td>\n",
       "      <td>t</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>s</td>\n",
       "      <td>00120</td>\n",
       "      <td>0</td>\n",
       "      <td>+</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>685</th>\n",
       "      <td>b</td>\n",
       "      <td>21.08</td>\n",
       "      <td>10.085</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>e</td>\n",
       "      <td>h</td>\n",
       "      <td>1.25</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00260</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>686</th>\n",
       "      <td>a</td>\n",
       "      <td>22.67</td>\n",
       "      <td>0.750</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>v</td>\n",
       "      <td>2.00</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>2</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00200</td>\n",
       "      <td>394</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>687</th>\n",
       "      <td>a</td>\n",
       "      <td>25.25</td>\n",
       "      <td>13.500</td>\n",
       "      <td>y</td>\n",
       "      <td>p</td>\n",
       "      <td>ff</td>\n",
       "      <td>ff</td>\n",
       "      <td>2.00</td>\n",
       "      <td>f</td>\n",
       "      <td>t</td>\n",
       "      <td>1</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00200</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>688</th>\n",
       "      <td>b</td>\n",
       "      <td>17.92</td>\n",
       "      <td>0.205</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>aa</td>\n",
       "      <td>v</td>\n",
       "      <td>0.04</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>f</td>\n",
       "      <td>g</td>\n",
       "      <td>00280</td>\n",
       "      <td>750</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>b</td>\n",
       "      <td>35.00</td>\n",
       "      <td>3.375</td>\n",
       "      <td>u</td>\n",
       "      <td>g</td>\n",
       "      <td>c</td>\n",
       "      <td>h</td>\n",
       "      <td>8.29</td>\n",
       "      <td>f</td>\n",
       "      <td>f</td>\n",
       "      <td>0</td>\n",
       "      <td>t</td>\n",
       "      <td>g</td>\n",
       "      <td>00000</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>690 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    A1     A2      A3 A4 A5  A6  A7    A8 A9 A10  A11 A12 A13    A14  A15 A16\n",
       "0    b  30.83   0.000  u  g   w   v  1.25  t   t    1   f   g  00202    0   +\n",
       "1    a  58.67   4.460  u  g   q   h  3.04  t   t    6   f   g  00043  560   +\n",
       "2    a  24.50   0.500  u  g   q   h  1.50  t   f    0   f   g  00280  824   +\n",
       "3    b  27.83   1.540  u  g   w   v  3.75  t   t    5   t   g  00100    3   +\n",
       "4    b  20.17   5.625  u  g   w   v  1.71  t   f    0   f   s  00120    0   +\n",
       "..  ..    ...     ... .. ..  ..  ..   ... ..  ..  ...  ..  ..    ...  ...  ..\n",
       "685  b  21.08  10.085  y  p   e   h  1.25  f   f    0   f   g  00260    0   -\n",
       "686  a  22.67   0.750  u  g   c   v  2.00  f   t    2   t   g  00200  394   -\n",
       "687  a  25.25  13.500  y  p  ff  ff  2.00  f   t    1   t   g  00200    1   -\n",
       "688  b  17.92   0.205  u  g  aa   v  0.04  f   f    0   f   g  00280  750   -\n",
       "689  b  35.00   3.375  u  g   c   h  8.29  f   f    0   t   g  00000    0   -\n",
       "\n",
       "[690 rows x 16 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "  drive_path\n",
    "except NameError:\n",
    "  drive_path = ''\n",
    "\n",
    "data_raw = pd.read_csv(drive_path + 'credit+approval/crx.data',header = None)\n",
    "data_columns = []\n",
    "for i in range(16):\n",
    "  data_columns.append('A' + str(i+1))\n",
    "data_raw.columns = data_columns\n",
    "display(data_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "-64j0cD9GHg1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b' 'a' '?']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "A1     12\n",
       "A2     12\n",
       "A3      0\n",
       "A4      6\n",
       "A5      6\n",
       "A6      9\n",
       "A7      9\n",
       "A8      0\n",
       "A9      0\n",
       "A10     0\n",
       "A11     0\n",
       "A12     0\n",
       "A13     0\n",
       "A14    13\n",
       "A15     0\n",
       "A16     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find out the representation of missing values used in the dataset\n",
    "# replace the representation with np.na \n",
    "print(data_raw1['A1'].unique()) #check column\n",
    "data_raw = data_raw.replace('?',np.nan) #replace ? to NaN\n",
    "\n",
    "# check the column with missing values and the amount\n",
    "data_raw.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8tssCrjdKu_8"
   },
   "outputs": [],
   "source": [
    "#convert column into the appropriate types\n",
    "\n",
    "data_raw[????] = pd.to_numeric(data_raw[????],errors=\"coerce\")\n",
    "data_raw.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HaUFhrAQKFhn"
   },
   "source": [
    "### Univariate Imputation\n",
    "Univariate imputation algorithm imputes values in the i-th attribute using only non-missing values in that attribute itself without considering other attributes in the dataset (no estimation using other attribtues).\n",
    "\n",
    "The `SimpleImputer` from scikit-learn provides basic strategies for this approach that can be applied to different data types, as long as the missing values are encoded as `np.nan`.\n",
    "\n",
    "Complete the codes below to perform the simple imputation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khUhTPsROsnY"
   },
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# initialize imputer to fill in missing values of categorical/object attribute\n",
    "# using the most frequently appearing categorical value\n",
    "cat_impute = SimpleImputer(missing_values=np.nan, strategy=????)\n",
    "\n",
    "#select all categorical / object attributes from the dataset\n",
    "data_cat = data_raw.select_dtypes(????)\n",
    "# fit the imputer with the selected categorical data columns and save the output\n",
    "# as a dataframe. Remember to name the columns again.\n",
    "data_cat_clean = pd.DataFrame(cat_impute.fit_transform(????))\n",
    "data_cat_clean.columns = data_cat.columns\n",
    "\n",
    "#Check the categorical data for any remaining missing values\n",
    "display(data_raw[cat_columns].isna().sum()) #before imputation\n",
    "display(data_cat_clean.isna().sum()) #after imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Ol0v3wwlII7"
   },
   "outputs": [],
   "source": [
    "#get index of rows with missing values\n",
    "data_cat_miss = []\n",
    "for i in data_cat.columns:\n",
    "  data_cat_miss.extend(data_cat[data_cat[i].isnull()].index)\n",
    "data_cat_miss = np.unique(data_cat_miss)\n",
    "#see imputed values\n",
    "display(data_cat_clean.iloc[data_cat_miss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w_0NhEUZMEyu"
   },
   "outputs": [],
   "source": [
    "# initialize imputer to fill in missing values of numerical attribute\n",
    "# using the means of the attributes\n",
    "num_impute = SimpleImputer(missing_values=np.nan,strategy=????)\n",
    "#select the numerical attributes from the dataset with missing values\n",
    "data_num = data_raw.select_dtypes(????)\n",
    "# fit the imputer with the selected numerical data columns and save the output\n",
    "# as a dataframe. Remember to name the columns again.\n",
    "data_num_clean = pd.DataFrame(num_impute.fit_transform(????))\n",
    "data_num_clean.columns = data_num.columns\n",
    "#Check the numerical data for any remaining missing values\n",
    "display(data_num.isna().sum())\n",
    "display(data_num_clean.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M48f8wGrkpkC"
   },
   "outputs": [],
   "source": [
    "#get index of rows with missing values\n",
    "data_num_miss = data_num[data_num['A2'].isnull()].index.tolist()\n",
    "#see imputed values\n",
    "display(data_num_clean.iloc[data_num_miss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "krFl8b0NObyn"
   },
   "outputs": [],
   "source": [
    "#create a copy of the dataset and name it data_clean\n",
    "data_clean = ????\n",
    "#replace the cleaned categorical and numerical columns into data_clean\n",
    "data_clean[data_cat_clean.columns] = ????\n",
    "data_clean[data_num_clean.columns] = ????\n",
    "data_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6AoRkfmKg2Fb"
   },
   "source": [
    "### Multivariate Imputation\n",
    "Mmultivariate imputation algorithms use the entire set of available attributes to estimate the missing values. Comparing to univariate imputation, this approach would be more sophisticated, providing more reasonable values for imputation. A convenient function provided but Scikit Learn for multivariate imputation is the `IterativeImputer`.\n",
    "\n",
    "Complete the code below to try out the `IterativeImputer`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FX6E1aRChk7A"
   },
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer #must be included to enable the usage of the imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# initialize imputer to fill in missing values of numerical attribute\n",
    "# using the means of the attributes\n",
    "num_iterimpute = IterativeImputer(random_state=0)\n",
    "#select the numerical attributes from the dataset with missing values\n",
    "data_num = ????\n",
    "# fit the imputer with the selected numerical data columns and save the output\n",
    "# as a dataframe. Remember to name the columns again.\n",
    "data_num_clean2 = pd.DataFrame(????.fit_transform(data_num))\n",
    "data_num_clean2.columns = ????\n",
    "#Check the numerical data for any remaining missing values\n",
    "display(data_num.isna().sum())\n",
    "display(data_num_clean2.isna().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EwnS9WiYjIbY"
   },
   "outputs": [],
   "source": [
    "#get index of rows with missing values\n",
    "data_num_miss = data_num[data_num['A2'].isnull()].index.tolist()\n",
    "#see imputed values\n",
    "display(data_num_clean2.iloc[data_num_miss]) #observe the difference with the results of the SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6F_BvPy6fgZ"
   },
   "source": [
    "Observe the difference in the imputer values between using the `SimpleImputer` and the `IterativeImputer`. There are also various parameters that can be set to adjust the imputation. Explore the documentation for these options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TsnNKG156c7s"
   },
   "outputs": [],
   "source": [
    "#replace the numerical columns in data_clean with the columns filled using IterativeImputer\n",
    "????\n",
    "\n",
    "data_clean.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-ozcglOy0ru"
   },
   "source": [
    "---\n",
    "\n",
    "## Dealing with duplicated data\n",
    "Recall: Do a quick check if there are any duplicated data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acJqk8aJy0rw"
   },
   "outputs": [],
   "source": [
    "# Enter your codes here to drop all the duplicated data...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i8MJd73py0rx"
   },
   "source": [
    "---\n",
    "## Dealing with inconsistencies and noisy data\n",
    "\n",
    "Other than missing and duplicated data, there can be many types of inconsistencies and noise in data that needs to be handled to ensure data quality. For instance, we know for a fact that some features are not suppose to have negative values, hence needs to be corrected (either by \"clipping\" the negative values and set to 0, or inferred and replaced with more reasonable values, or etc.). Simple cases like this can be identified and handled using simple programming depending on the data structure.\n",
    "\n",
    "Try the following toy example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtLSWCRLy0rz"
   },
   "outputs": [],
   "source": [
    "# Simple dataset of age and income with noisy data records\n",
    "age = pd.Series([24,21,24,-26,27])\n",
    "income = pd.Series([3000,2500,3400,4599, -9999])\n",
    "\n",
    "df = pd.DataFrame({\"age\":age, \"income\":income})\n",
    "\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wqCbB2-1y0r2"
   },
   "outputs": [],
   "source": [
    "# Replace negative values with nan for age and income\n",
    "\n",
    "age = age.map(lambda x: np.nan if x < 0 else x)\n",
    "income = income.map(lambda x: np.nan if x < 0 else x)\n",
    "\n",
    "display(age)\n",
    "display(income)\n",
    "\n",
    "#The values can then be imputed using the SimpleImputer or IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkxU2eD75Fxt"
   },
   "outputs": [],
   "source": [
    "#Alternatively, can clip the values if the appropriate range is known\n",
    "df = df.clip(lower=0)\n",
    "df = df.clip(upper=5000)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mF-W98b__Ri0"
   },
   "source": [
    "Noise and inconsistencies could happen for categorical data as well. Let's try cleaning the categorical columns in the Credit Approval dataset. For practise purposes, let's make the following few assumptions for this exercise:\n",
    "* Consider the attributes with 3 unique values\n",
    "* Assume the value with significantly small counts / frequency to be noise\n",
    "* Clean by replacing those values appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WbLg2Qp55w07"
   },
   "outputs": [],
   "source": [
    "# Check the categorial (object) attributes in the previously cleaned dataset\n",
    "# hint: use describe only for that type of data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kLm2SfwA_hQY"
   },
   "outputs": [],
   "source": [
    "# Identify the attributes with 3 unique values and find out the frequencies (can use visualization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bX64bOsMFARN"
   },
   "outputs": [],
   "source": [
    "# replace the data with most frequent category\n",
    "\n",
    "\n",
    "\n",
    "for i in range((len(cat_columns))):\n",
    " sns.catplot(data = data_clean, x = cat_columns[i], kind='count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnTzaGVEG-nr"
   },
   "source": [
    "Other than using the most frequent count, there are other methods that can be used to infer values to clean noisy data (and also fill in missing values), such as **Hot Deck Imputation**, modified **KNN imputation**, **Predictive model inference**, etc.\n",
    "\n",
    "It is also a common issue encountered in real world datasets where **strings contain unnecessary characters, symbols or white spaces**. Such issues have vert large variety that we cannot manually list out all the texts to be replaced. So, for such data problems, Regular Expressions (identifying patterns in the text) is very useful for the cleaning.\n",
    "\n",
    "`regex` is a parameter used in the `replace()` function that determines if the passed-in pattern is a regular expression. If True, it assumes the passed-in pattern is a regular expression and, if False, the function treats the pattern as a literal string.\n",
    "\n",
    "Try the following test sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UYpep-74y0r6"
   },
   "outputs": [],
   "source": [
    "state = pd.Series([\"\\tJohor,Bahru\\n\\t\", \"Sela   ngor\\n\\t\", \" Sabah     \", \"Sarawak`\", \"Penang\", \"123Kel3antan4\", \"Ke_dah\", \"Pahan?g\"])\n",
    "display(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oEtStjfuy0r7"
   },
   "outputs": [],
   "source": [
    "#remove unnecessary characters and spaces\n",
    "state = state.replace(\"[^a-zA-Z]\", \"\", regex=True)\n",
    "display(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jQg5gTIMy0r7"
   },
   "outputs": [],
   "source": [
    "# Regular Expressions (regex) is super useful in cleaning out strings.\n",
    "state = state.replace(\"\\s\", \"\", regex=True) # remove all whitespace, works even if the whitespace is in the center of a string as opposed to state.str.strip()\n",
    "\n",
    "# there are many ways we can clean this data using regex, since we know the state only consist of alphabets we can do:\n",
    "state = state.replace(\"[^a-zA-Z]\", \"\", regex=True).replace(\"JohorBahru\", \"Johor Bahru\")\n",
    "\n",
    "display(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fo5Hn_Ayy0sj"
   },
   "source": [
    "## Intergrating datasets\n",
    "Often in data science / data mining projects, sets of data may originate from various different sources. In order for efficient and effective extraction of knowledge, these sets need to be merged into a single unified set.\n",
    "\n",
    "In the simplest case, datasets organized as relational tables can be intergrated easily in terms of column or rows. We can use `pandas.merge()` to do a join (inner, outer, left, right) by key (identified linking various tables) or simply use `pandas.concat()` to concatenate by row or column.\n",
    "\n",
    "Reference for different type of joining:\n",
    "* *Inner join*: return the common rows between the two tables\n",
    "* *Outer join*: return the common rows between the two tables and the rows which are not matched\n",
    "* *Left join*: returns all records from the left side and matched rows from the right table\n",
    "* *Right join*: returns all rows from the right side and unmatched rows from the left table\n",
    "\n",
    "Try the following dummy data for a quick test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cHLqjTzuy0sk"
   },
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'key': ['key_1', 'key_2', 'key_3'],\n",
    "                   'B': np.random.random_sample(3)*1000})\n",
    "\n",
    "df2 = pd.DataFrame({'key': ['key_1', 'key_2', 'key_3', 'key_4'],\n",
    "                   'C': np.random.random_sample(4)*100})\n",
    "\n",
    "display(df1)\n",
    "display(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LwI4XowZy0sn"
   },
   "outputs": [],
   "source": [
    "# Merging by key, try out the different merges using the 'how' parameter\n",
    "df1.merge(df2, on=\"key\", how=\"outer\") # same concept as sql joins\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "242UXdv6y0so"
   },
   "outputs": [],
   "source": [
    "#concat will simply add the data in\n",
    "display(pd.concat([df1,df2], sort=True, axis=0)) # by row\n",
    "display(pd.concat([df1,df2], sort=True, axis=1)) # by column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ14YpiiXLjU"
   },
   "source": [
    "Notice the `NaN` cells created during the merging as the two sets of data may not have the exact match of rows and columns.\n",
    "\n",
    "Next, try out the merging on the Big Mart Sales data used in the previous tutorial. Recall that one of the set is lacking one column. Consider what type of merging is appropriate for that set, and implement using code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XnwgiFZ-Xdiv"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(drive_path + \"BigMartSales/train.csv\")\n",
    "df_test = pd.read_csv(drive_path + \"BigMartSales/test.csv\")\n",
    "# Enter your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyaAh10xX7h2"
   },
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "After cleaning and merging processes, there may / may not be some attributes that are closely related.\n",
    "\n",
    "### Chi-squared Test (categorical data)\n",
    "\n",
    "Firstly, let's try the **chi-squared test** from `scipy` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v5SNW15Dy0sz"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# create contingency table of the 2 attributes\n",
    "ct_table_ind=pd.crosstab(data_clean['A1'],data_clean['A4'])\n",
    "print('contingency_table :\\n',ct_table_ind)\n",
    "\n",
    "# compute the chi2 stat and get the value\n",
    "c_stat, p, dof, expected = chi2_contingency(ct_table_ind)\n",
    "\n",
    "print(\"Chi2 statistic: \", c_stat)\n",
    "# interpret p-value\n",
    "alpha = 0.05\n",
    "print(\"p value is \" + str(p))\n",
    "if p <= alpha:\n",
    "    print('Dependent (reject H0)')\n",
    "else:\n",
    "    print('Independent (H0 holds true)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHpkupMjc8e9"
   },
   "source": [
    "Exercise: Compute the chi2 statistic for all pairs of the categorical attributes. Create a heatmap to visualize the relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0WnL8Akgdx3h"
   },
   "outputs": [],
   "source": [
    "#Enter your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4mg4U5ty0s1"
   },
   "source": [
    "### Covariance (Numerical)\n",
    "\n",
    "We can find relationships of numerical data using covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hTLlKA15bXjf"
   },
   "outputs": [],
   "source": [
    "cov_mat = data_clean.cov(numeric_only=True)\n",
    "display(cov_mat)\n",
    "sns.heatmap(cov_mat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NV2nLXndy0s1"
   },
   "source": [
    "Notice the covariance values are significantly different for the different attributes, which may be misleading when assessing relationships of attributes with different ranges in their units.\n",
    "\n",
    "### Correlation Coefficient\n",
    "Lastly, let's try the correlation coefficient for these numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XgAOvO0fy0s4"
   },
   "outputs": [],
   "source": [
    "corr_matrix  = data_clean.corr(numeric_only = True)\n",
    "sns.heatmap(corr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Qs1fDfic1mJ"
   },
   "source": [
    "Notice the more meaningful heatmap showing the relationship of the numerical attributes due to the normalization. There are various methods of correlation provided by pandas which can be selected using the `methods` parameter. Explore the documentation for the options.\n",
    "\n",
    "Lastly, based on the various relationship analysis, what can be concluded from the Credit Approval dataset attributes? Write down your observations:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1G3-wGteXDp"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
