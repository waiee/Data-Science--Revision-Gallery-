{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOn9i84jwOKpaCyJXa+zIOF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"N3NATOdUWtK6"},"source":["<img src=\"https://www.mmu.edu.my/fci/wp-content/uploads/2021/01/FCI_wNEW_MMU_LOGO.png\" style=\"height: 80px;\" align=left>  "]},{"cell_type":"markdown","metadata":{"id":"FIsHUpw1WtK8"},"source":["# Learning Objectives\n","\n","Towards the end of this lesson, you should be able to:\n","- perform clustering using K-means\n","- evaluate clustering results using external measures\n"]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","### For Google Colab Use Only\n","Skip this section if you are using Jupyter Notebook etc."],"metadata":{"id":"6RNqOK-ccT6i"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"DE6XffsuclLs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["drive_path = '/content/drive/MyDrive/Trimester/2310/TDS3301/Tutorials/Tutorial 10/' #set your google drive path"],"metadata":{"id":"Jcx6xDmdco5S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"OwhnnLuhQ1K7"}},{"cell_type":"markdown","source":["# Cluster Analysis\n","Clustering or data segmentation is the task to partition a given set of data points (dataset) into a set of groups (i.e., clusters) which are as similar as possible. It is fundamentally an unsupervised learning task with no predefined classes to guide the grouping.\n","\n","The key to clustering is similarity / dissimilarity of the data points. A good clustering outcome has 2 criteria:\n","1. Data points within / inside the same cluster has high similarity / low distance to indicate a cohesive cluster (high intra-class similarity).\n","2. Data points between / across different clusters has low similarity / high distance to indicate distinctive cluster (low inter-class similarity).\n","\n","## k-Means Clustering\n","\n","k-Means is a partition type clustering algorithm. Using similarity/distrance measures, data points are clustered based on centroids that are iteratively updated. Comparatively, k-Means is an efficient clustering method to produce convex-shaped groups of data.\n","\n","The [`scikit-Learn`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans) package provides 2 versions of the approach, the general k-Means, and a modified version called, Bisecting k-Means.\n","\n","The following experiments lets you try out k-Means clustering on the `diabetes.csv` dataset that has been used various times previously.\n","\n","---\n","\n","## Data Preprocessing\n","\n","The following steps of preprocessing are replicated from the previous tutorial."],"metadata":{"id":"Z_kpIbOOAJVP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"_4i41P3k_JC7"},"outputs":[],"source":["#import the relevant libraries\n","import pandas as pd #data reading\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import StandardScaler # label encoding\n","from sklearn.model_selection import train_test_split # Import train_test_split function\n","import seaborn as sns\n","import numpy as np\n","\n","#code to ensure path\n","try:\n","  drive_path\n","except NameError:\n","  drive_path = ''\n","\n","# load diabetes.csv dataset\n","df = pd.read_csv(drive_path + \"diabetes.csv\")\n","df.head()"]},{"cell_type":"markdown","source":["The `diabetes.csv` dataset contains fully numerical data. <br>\n"],"metadata":{"id":"LKQYjhCOEnlZ"}},{"cell_type":"code","source":["# check the data types\n","df.dtypes"],"metadata":{"id":"U8maChL1FnIf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Additionally, let's have a look at the distribution among the various attributes to see of at apparent partitioning."],"metadata":{"id":"spTLAGIk9XHx"}},{"cell_type":"code","source":["sns.relplot(x=\"BMI\", y=\"Glucose\", hue=\"Outcome\", data=df)\n","plt.show()"],"metadata":{"id":"72phMooL8CHC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For clustering, we do not need target classes for training, however, we still use the class labels in 2 ways:\n","1. As another attribute for clustering (exploring inherent data groupings)\n","2. As reference groundtruth to evaluate clustering performance (external measures)\n","\n","We will be using class labels for evaluation later, so let's split the dataset attributes and class labels into their own variables, and then create the training, validation, and testing sets as we did in classification tasks. <br>\n","*Note*: The `train_test_split` function does not provide a direct train/val/test split, so we can split the dataset twice to create the 3 sets."],"metadata":{"id":"yiKG4c_mHZZr"}},{"cell_type":"code","source":["#split dataset in features and target variable*\n","# feature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age','Glucose','BloodPressure','DiabetesPedigreeFunction']\n","X = df.drop('Outcome',axis=1) #Features\n","y = df.Outcome # Target variable\n","print(X.shape)\n","print(y.shape)"],"metadata":{"id":"vyUDgBUzHKhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#First split to get 20% as test set\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1, stratify = y)\n","#Split the training set again to get the validation set (requires calculation the get the needed percentage)\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.125, random_state=1, stratify=y_train)\n","# 12.5% of 80% is 10% of the whole dataset (0.125 x 0.8 = 0.1)\n","\n","print('Training set: ', y_train.shape)\n","print('Validation set: ', y_val.shape)\n","print('Testing set: ', y_test.shape)\n"],"metadata":{"id":"diMdTDQKIG9T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As k-Means clustering uses similarity / distance measures, the training data needs to be normalized as well to ensure no bias in the features during model training."],"metadata":{"id":"dyk5Jhd5T-HF"}},{"cell_type":"code","source":["#normalize the training data\n","scaler = StandardScaler().fit(X_train)\n","X_train_norm = scaler.transform(X_train)\n"],"metadata":{"id":"7ID7TzFZULDE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["End of data preprocessing\n","\n","---\n","\n","## Training and Evaluation\n","\n","With the ready datasets, we can create and train a k-Means model. To start, let's setup the k-Means clustering method with 2 clusters.<br>\n","By default, the implementation uses `k-means++` initialization."],"metadata":{"id":"6GRi5ZyTMaCi"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","\n","# the number of clusters here is 2\n","\n","km = KMeans(n_clusters = 2, random_state=1)\n","km.fit(X_train_norm)"],"metadata":{"id":"KanHwBXJqNsk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["After training, we can extract the \"labels\" from the model. These labels are not exactly the classes as normally produced by classifiers. The labels here are mainly indicating the clusters produced. To actually identify the \"classes\", we would need to map them based on majority by referring to the target classes of the dataset (`y`) later."],"metadata":{"id":"PWJd9FSN-upn"}},{"cell_type":"code","source":["print(km.labels_)"],"metadata":{"id":"HQc-qV5v-rxO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For now, let's visualize the clustered outcome again to observe the differences. Firstly, merge the cluster labels with the training attributes:"],"metadata":{"id":"9dPog6IG_feZ"}},{"cell_type":"code","source":["#Merge training set\n","df_train = X_train.copy()\n","df_train['Outcome']=y_train\n","#Merge cluster set\n","df_train_clustered = X_train.copy()\n","df_train_clustered['Cluster_kmeans']=km.labels_"],"metadata":{"id":"M-bv2Cxg_8ag"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualize the ground truth labels of the training data and the clustered labels side-by-side for comparison."],"metadata":{"id":"AWy0mEE4CR56"}},{"cell_type":"code","source":["_, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Outcome\", data=df_train, ax=axes[0]).set( title='Ground truth')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster_kmeans\", data=df_train_clustered,ax=axes[1]).set(title='Clustering')\n","plt.show()"],"metadata":{"id":"zuilWiVu-1g2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that the most of the data points have been clustered similarly to the original data.\n","\n","To use the trained model, just apply it using `predict`."],"metadata":{"id":"3z8rs6gTTC29"}},{"cell_type":"code","source":["X_val_norm = scaler.transform(X_val)\n","val_cluster = km.predict(X_val_norm)\n","\n","#Merge training set\n","df_val = X_val.copy()\n","df_val['Outcome']=y_val\n","#Merge cluster set\n","df_val_clustered = X_val.copy()\n","df_val_clustered['Cluster']=val_cluster\n","\n","_, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Outcome\", data=df_val, ax=axes[0]).set( title='Validation Ground Truth')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster\", data=df_val_clustered,ax=axes[1]).set(title='Validation Clustering')\n","plt.show()"],"metadata":{"id":"zoIKOaLLTOBI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n","### Determining k value\n","\n","Unlike the previous example, clustering may also be done without any reference labels in order to explore for unknown groupings within data. In such cases, determining the k for partition-based clustering can be crucial.\n","\n","A popular approach to infer the optimum k is using the \"Elbow Method\". Multiple k-Means models are trained using different k values, and the resultant Sum of Squared Distances (SSD) are used to generate a plot of SSD vs. k. Using the plot, we identify the best k at the \"elbow\".\n","\n","Let's identify the best k from the previous dataset by testing 1 to 20 clusters.We can extract the SSD from the trained k-Means model from the `inertia` attributes."],"metadata":{"id":"D0OuaQVtCimN"}},{"cell_type":"code","source":["max_k = 20\n","ssd = []\n","for i in range(1, max_k+1):\n","    km_elbow = KMeans(\n","        n_clusters=i, init='random',\n","        n_init=10, max_iter=300,\n","        tol=1e-04, random_state=0\n","    )\n","    km_elbow.fit(X_train_norm)\n","    ssd.append(km_elbow.inertia_)#Sum of squared distances of samples to their closest cluster center\n","\n","# plot\n","plt.plot(range(1, max_k+1), ssd, marker='o')\n","plt.xticks(range(1, max_k+1))\n","plt.xlabel('Number of clusters')\n","plt.ylabel('SSD')\n","plt.show()"],"metadata":{"id":"OSOfrjFOJe7q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The best k for this case seem to be 7."],"metadata":{"id":"Xl1jzUB8Nfr5"}},{"cell_type":"code","source":["km_best = KMeans(n_clusters = 7, random_state=1, n_init = 10)\n","km_best.fit(X_train_norm)\n","\n","#Merge cluster set\n","df_best = X_train.copy()\n","df_best['Cluster']=km_best.labels_\n","\n","_, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Outcome\", data=df_train, ax=axes[0], palette='cool',edgecolor=\"black\").set( title='Ground truth')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster\", data=df_best,ax=axes[1], palette='cool',edgecolor=\"black\").set(title='Clustering')\n","plt.show()"],"metadata":{"id":"2zlMZ1ReLobV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","### pyclustering Package\n","\n","The k-Means implementation in scikit-learn is very basic and it does not provide much flexibility in tuning parameters. For instance, the distance metric is hard-set as Euclidean Distance. For alternative methods, there are various other packages that provides more flexibility and also implementation of other variants of this clustering approach.\n","\n","Let's try the [`pyclustering` package](https://pyclustering.github.io/docs/0.10.1/html/index.html):\n","\n"],"metadata":{"id":"zRQMJppBQkA8"}},{"cell_type":"code","source":["!pip install pyclustering"],"metadata":{"id":"uBFzJfD5QjQu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, we are going to re-implement the same process of the k-Means using `pyclustering`.<br>\n","The general process is exactly the same, just different syntax:"],"metadata":{"id":"mAjj4ki4hvqT"}},{"cell_type":"code","source":["from pyclustering.cluster.kmeans import kmeans\n","from pyclustering.cluster.center_initializer import kmeans_plusplus_initializer\n","# Prepare initial centers using K-Means++ method.\n","initial_centers = kmeans_plusplus_initializer(X_train_norm, 2).initialize()\n","\n","# Create instance of K-Means algorithm with prepared centers.\n","kmeans_instance = kmeans(X_train_norm, initial_centers,ccore=False)\n","\n","# Run cluster analysis and obtain results.\n","kmeans_instance.process()\n","clusters = kmeans_instance.get_clusters()\n","final_centers = kmeans_instance.get_centers()\n","\n","print(initial_centers)\n","print(clusters)\n","print(final_centers)"],"metadata":{"id":"bGoYt1ZNAmlZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The clustering output (`clusters`) of the k-Means using pyclustering is formatted differently from scikit-learn. Instead of the full list of labels, it returns the indices of clustered data points, sorted in lists.\n","\n","For quick conversion, we can use the function below:"],"metadata":{"id":"15ye49vHf5o8"}},{"cell_type":"code","source":["def cluster_pyc2skl(labels_list,py_clusters):\n","  cluster_skform = np.zeros(len(labels_list))\n","  cluster_list = range(len(py_clusters))\n","  cluster_num = 0\n","  for i in py_clusters:\n","    for j in i:\n","      cluster_skform[j] = cluster_num\n","    cluster_num += 1\n","  return cluster_skform\n","\n","pyc_cluster = cluster_pyc2skl(y_train,clusters)\n","print(pyc_cluster)"],"metadata":{"id":"fd5WmqS-bXun"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can merge with the attributes and visualize the clustering using this alternative package:"],"metadata":{"id":"zkAT_6jAhSNW"}},{"cell_type":"code","source":["#Append results to the dataframe for visualization\n","df_train_clustered['Cluster_pyc']= pyc_cluster\n","\n","_, axes = plt.subplots(nrows=1, ncols=2, figsize=(12,5))\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Outcome\", data=df_train, ax=axes[0]).set( title='Ground truth')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster_pyc\", data=df_train_clustered,ax=axes[1]).set(title='Clustering')\n","plt.show()"],"metadata":{"id":"p3RlrTVPdadp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Notice that the outcome is similar (minor differences due to intialization).<br> Let's check the validation set:"],"metadata":{"id":"fIrKQDlYT41p"}},{"cell_type":"code","source":["val_pyc_clusters = kmeans_instance.predict(X_val_norm)\n","\n","#Merge cluster set\n","df_val_clustered['Cluster_pyc']=val_pyc_clusters\n","\n","_, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,5))\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Outcome\", data=df_val, ax=axes[0]).set( title='Validation Ground Truth')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster\", data=df_val_clustered,ax=axes[1]).set(title='Validation Clustering (scikit-learn)')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster_pyc\", data=df_val_clustered,ax=axes[2]).set(title='Validation Clustering (pyclustering)')\n","plt.show()"],"metadata":{"id":"IbEs0HKGV7yB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For different outcomes, we have the flexibility to change the [distant metric](https://pyclustering.github.io/docs/0.10.1/html/df/df9/classpyclustering_1_1utils_1_1metric_1_1distance__metric.html) using this package, for instance using the Minkowski distance formulation for different degree of distances.\n","\n","The following is an example of implementing k-means using different distance:\n"],"metadata":{"id":"kCaKGEcbhbDA"}},{"cell_type":"code","source":["from pyclustering.utils.metric import distance_metric, type_metric #import methods for metrics\n","metric = distance_metric(type_metric.MINKOWSKI, degree=5)\n","\n","# create instance of K-Means using specific distance metric:\n","kmeans_minkowski = kmeans(X_train_norm, initial_centers, metric=metric) #using same initial centers from previous test\n","\n","# run cluster analysis and obtain results\n","kmeans_minkowski.process()\n","clusters_minkowski = kmeans_minkowski.get_clusters()\n","\n","# convert clustering outcomes to sklearn format\n","clusters_minkowski = cluster_pyc2skl(y_train,clusters_minkowski)\n","\n","# visualize outcomes\n","df_train_clustered['Cluster_Minkowski(5)']= clusters_minkowski\n","\n","_, axes = plt.subplots(nrows=1, ncols=3, figsize=(20,5))\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Outcome\", data=df_train, ax=axes[0]).set( title='Ground truth')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster_pyc\", data=df_train_clustered,ax=axes[1]).set(title='Clustering (Euclidean)')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster_Minkowski(5)\", data=df_train_clustered,ax=axes[2]).set(title='Clustering (Minkowski (5))')\n","plt.show()"],"metadata":{"id":"mCsRkRFgfuDT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The clusters may change significantly when different metrics are used.\n","\n","---\n","\n","## Variants of k-Means\n","\n","While k-Means by itself is very efficient and mostly useful, it is prone to outliers, as noticeable from the previous examples. `pyclustering` provides the k-Medoids and k-Medians version of the method which are not available in scikit-learn.\n","\n","### k-Medoids\n","The k-Medoids appraoch to handle the outlier issue in k-Means but using actual data points as centroids, instead of the computed value.\n","\n","Let's re-implement the previous clustering and compare them:"],"metadata":{"id":"Vr68xTrm_Sa9"}},{"cell_type":"code","source":["from pyclustering.cluster.kmedoids import kmedoids\n","\n","# Initialize initial medoids using K-Means++ algorithm\n","initial_medoids = kmeans_plusplus_initializer(X_train_norm, 2).initialize(return_index=True)\n","\n","# Create instance of K-Medoids (PAM) algorithm.\n","kmedoids_instance = kmedoids(X_train_norm, initial_medoids)\n","\n","# Run cluster analysis and obtain results.\n","kmedoids_instance.process()\n","clusters_medoids = kmedoids_instance.get_clusters()\n","medoids = kmedoids_instance.get_medoids()\n","\n","# convert clustering outcomes to sklearn format\n","clusters_medoids = cluster_pyc2skl(y_train,clusters_medoids)\n","\n","# visualize outcomes\n","df_train_clustered['Cluster_Medoids']= clusters_medoids\n","\n","_, axes = plt.subplots(nrows=2, ncols=2, figsize=(12,12))\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Outcome\", data=df_train, ax=axes[0,0]).set( title='Ground truth')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster_pyc\", data=df_train_clustered,ax=axes[0,1]).set(title='Clustering (Euclidean)')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster_Minkowski(5)\", data=df_train_clustered,ax=axes[1,0]).set(title='Clustering (Minkowski (5))')\n","sns.scatterplot(x=\"BMI\", y=\"Glucose\", hue=\"Cluster_Medoids\", data=df_train_clustered,ax=axes[1,1]).set(title='Clustering (Medoids)')\n","plt.show()\n","\n"],"metadata":{"id":"Gx9KTUc4_GWM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The package also provides other methods. Feel free to explore them in the documentations.\n","\n","---\n","\n","### Exercise\n","Cluster the testing set using all the previously trained models, and visualize the results."],"metadata":{"id":"-c9xhgvEPLXC"}},{"cell_type":"code","source":[],"metadata":{"id":"lgX7Kp8JPeaR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","---\n","\n","## Clustering Evaluations\n","\n","Evaluation is a very important aspect of data mining, and machine learning. Clustering outcomes can be evaluated in various ways, with of without reference labels / ground truth. We will now explore the evaluation metrics that makes use of the dataset target labels / ground truth.\n","\n","\n","### Rand Index\n","Rand index is a function that measures the similarity of the two assignments, ignoring permutations, i.e. it does not consider the position / index of the clustered points' labels (for classification, permutation must be included to ensure correct samples are being compared). Rand index outputs evaluation scores with the range of [0,1] where 1 indicates perfect clustering outcome.\n","\n","Let's compare all the method's training outcomes using Rand Index:"],"metadata":{"id":"x0H6893TEehX"}},{"cell_type":"code","source":["from sklearn.metrics import rand_score\n","\n","rand_skl_kmeans = rand_score(y_train, km.labels_)\n","rand_pyc_kmeans = rand_score(y_train, pyc_cluster)\n","rand_minkowski_kmeans = rand_score(y_train, clusters_minkowski)\n","rand_kmedoids = rand_score(y_train,clusters_medoids)\n","\n","print('Rand index')\n","print('k-Means: Euclidean (scikit-learn): ',rand_skl_kmeans)\n","print('k-Means: Euclidean (pyclustering): ',rand_pyc_kmeans)\n","print('k-Means: Minkowski5 (pyclustering): ',rand_minkowski_kmeans)\n","print('k-Medoids (pyclustering): ',rand_kmedoids)"],"metadata":{"id":"4wC0UeVQEdtm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["What is the best performing method?\n","\n","### Homogeinity, Completeness, and V-measure\n","Alternatively, the ground truth is also commonly used to evaluate the extent of match between the assigned clusters of the data points with the given target labels.\n","\n","In particular, the 3 metrics can be used:\n","* **Homogeneity**: each cluster contains only members of a single class.\n","* **Completeness**: all members of a given class are assigned to the same cluster.\n","* **V-measure**: The harmonic mean of homogeneity and completeness.\n","\n","All 3 measures are in the range of [0,1] where closer to 1 indicates better performance. <br>\n","Try them out on the training set:"],"metadata":{"id":"_tA9Z7KHLvsy"}},{"cell_type":"code","source":["from sklearn.metrics import homogeneity_score\n","\n","homo_skl_kmeans = homogeneity_score(y_train, km.labels_)\n","homo_pyc_kmeans = homogeneity_score(y_train, pyc_cluster)\n","homo_minkowski_kmeans = homogeneity_score(y_train, clusters_minkowski)\n","homo_kmedoids = homogeneity_score(y_train,clusters_medoids)\n","\n","print('Homogeneity Score')\n","print('k-Means: Euclidean (scikit-learn): ',homo_skl_kmeans)\n","print('k-Means: Euclidean (pyclustering): ',homo_pyc_kmeans)\n","print('k-Means: Minkowski5 (pyclustering): ',homo_minkowski_kmeans)\n","print('k-Medoids (pyclustering): ',homo_kmedoids)"],"metadata":{"id":"tfrZXmOMCrv6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import completeness_score\n","\n","comp_skl_kmeans = completeness_score(y_train, km.labels_)\n","comp_pyc_kmeans = completeness_score(y_train, pyc_cluster)\n","comp_minkowski_kmeans = completeness_score(y_train, clusters_minkowski)\n","comp_kmedoids = completeness_score(y_train,clusters_medoids)\n","\n","print('Completeness Score')\n","print('k-Means: Euclidean (scikit-learn): ',comp_skl_kmeans)\n","print('k-Means: Euclidean (pyclustering): ',comp_pyc_kmeans)\n","print('k-Means: Minkowski5 (pyclustering): ',comp_minkowski_kmeans)\n","print('k-Medoids (pyclustering): ',comp_kmedoids)"],"metadata":{"id":"4kUpAqD9GyzB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import v_measure_score\n","\n","v_skl_kmeans = v_measure_score(y_train, km.labels_)\n","v_pyc_kmeans = v_measure_score(y_train, pyc_cluster)\n","v_minkowski_kmeans = v_measure_score(y_train, clusters_minkowski)\n","v_kmedoids = v_measure_score(y_train,clusters_medoids)\n","\n","print('V-measure Score')\n","print('k-Means: Euclidean (scikit-learn): ',v_skl_kmeans)\n","print('k-Means: Euclidean (pyclustering): ',v_pyc_kmeans)\n","print('k-Means: Minkowski5 (pyclustering): ',v_minkowski_kmeans)\n","print('k-Medoids (pyclustering): ',v_kmedoids)"],"metadata":{"id":"RAUphZwHQ6tA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### Fowlkes-Mallows scores\n","\n","Lastly, the Fowlkes-Mallows score (FMI) that is also an external metric that uses the ground truth labels to evaluate clustering outcome. The score is defined as the geometric mean of the pairwise precision and recall, where:\n","* True Positive: the number of pair of points that belong to the same clusters in both the true labels and the predicted labels)\n","* False Positive: the number of pair of points that belong to the same clusters in the true labels and not in the predicted labels\n","* False Negative: the number of pair of points that belongs in the same clusters in the predicted labels and not in the true labels).\n","\n","The score ranges between [0,1] where 1 indicates a good clustering.\n","\n","Try the code:"],"metadata":{"id":"vXnN9vLhRdIT"}},{"cell_type":"code","source":["from sklearn.metrics import fowlkes_mallows_score\n","\n","fm_skl_kmeans = fowlkes_mallows_score(y_train, km.labels_)\n","fm_pyc_kmeans = fowlkes_mallows_score(y_train, pyc_cluster)\n","fm_minkowski_kmeans = fowlkes_mallows_score(y_train, clusters_minkowski)\n","fm_kmedoids = fowlkes_mallows_score(y_train,clusters_medoids)\n","\n","print('V-measure Score')\n","print('k-Means: Euclidean (scikit-learn): ',fm_skl_kmeans)\n","print('k-Means: Euclidean (pyclustering): ',fm_pyc_kmeans)\n","print('k-Means: Minkowski5 (pyclustering): ',fm_minkowski_kmeans)\n","print('k-Medoids (pyclustering): ',fm_kmedoids)"],"metadata":{"id":"J3eVO7NuRGkO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each measure would reveal a different aspect of the results and provides further understanding on a models' performance. Understanding what each metric represents and analysing the scores of each model can yeild useful analysis outcomes, or even reveal possible shortcomings in models for improvement.\n","\n","---\n","\n","### Exercise\n","Evaluated all the testing results produced in the previous exercise. Which model performed the best?"],"metadata":{"id":"D0u3-OOIS-cU"}},{"cell_type":"code","source":[],"metadata":{"id":"uI_nc-q1S8lg"},"execution_count":null,"outputs":[]}]}